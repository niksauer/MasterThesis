%!TEX root = ../main.tex

\chapter{Theoretical Framework}
\label{chp:theoretical-framework}

\section{Microservice Architecture}
\label{sec:microservice-architecture}

Popularized by companies like Amazon, Netflix, Uber, LinkedIn and SoundCloud, the microservice architecture has emerged as a pattern to avoid the problems of conventional monolithic designs \cite[p.~847]{singh2017container} \cite[p.~584]{villamizar2015evaluating}. This section provides an overview to the problems faced in monolithic applications, distinguishes the microservice architecture from a traditional \acl{SOA} and lays out its core principles, while noting some of the newly introduced challenges.


\subsection{The Monolith Problem}
\label{sec:monolith-problem}

A monolith is a software application whose modules cannot be executed independently \cite[p.~1]{dragoni2017microservices}. Hence, a monolith is characterized by requiring to be deployed as a united solution \cite[p.~24]{dmitry2014micro}. Based on this definition, a set of obstructive characteristics inherent to monolithic applications can be derived:

\begin{description}
  \item[Maintainability]
  \hfill \\
  When developing an application with a single large codebase, it naturally becomes harder to maintain and comprehend \cite[p.~2]{dragoni2017microservices}. The latter is especially true for beginners, slowing down their productivity \cite[p.~24]{dmitry2014micro}. Further, refactoring changes may touch many parts of the software which might lead to a situation in which refactoring is ignored because it becomes too risky \cite[p.~35]{kalske2017challenges}.

 \item[Dependencies and technology lock-in]
  \hfill \\
  Monoliths typically suffer from the \enquote{dependency hell} problem where an application's modules depend on conflicting versions of a shared library \cite[p.~2]{dragoni2017microservices}. In cases where this is solved, the likelihood of having to make many changes to update to a newer version of a library again increases with a growing codebase. Next to that, it becomes very difficult to change the technology stack, leading to a lock-in and forcing developers to use the same language in every problem domain \cite[p.~24]{dmitry2014micro} \cite[p.~2]{dragoni2017microservices}.

  \item[Deployment]
  \hfill \\
  Rolling out a new application version requires the complete set of services to be restarted, regardless of whether a service has been altered or not \cite[p.~2]{dragoni2017microservices}. Similarly, failure of one service leads to downtime across all services \cite[p.~970]{vayghan2018deploying}. The deployment can therefore be viewed as a single point of failure \cite[p.~584]{villamizar2015evaluating}. Moreover, deployments are likely sub-optimal due to conflicting resource requirements (e.g. \acs{CPU} vs. memory-intensive). Developers often have to compromise with a one-size fits all configuration \cite[p.~2]{dragoni2017microservices}.

  \item[Scaling]
  \hfill \\
  By combining multiple services into a single process, scaling can lead to resource wastage since the whole application needs to be scaled up even if an increase in traffic stresses only a subset of modules \cite[p.~850]{singh2017container} \cite[p.~2]{dragoni2017microservices}. Less popular services consume unnecessary (idle) amounts of resources \cite[p.~584]{villamizar2015evaluating}. Setting appropriate scaling thresholds also becomes more challenging because different components may have different resource requirements \cite[p.~24]{dmitry2014micro}.
\end{description}

\citeauthor{kalske2017challenges}, however, acknowledge that the monolith approach might be the correct choice if the codebase is relatively small or the need for fine-grained scaling has not come up \cite[pp.~34,~36]{kalske2017challenges}. \citeauthor{villamizar2015evaluating} add that monoliths are faster to set up \cite[p.~589]{villamizar2015evaluating}. Empirically, most organizations start with something big and slowly transition to a decomposed architecture when scaling problems arise \cite[p.~113]{thones2015microservices} \cite[p.~590]{villamizar2015evaluating}. Yet, it can be argued that an organization should spend more time on the design upfront since it is easy to introduce tight coupling, hereby hindering future refactoring endeavors \cite[p.~34]{kalske2017challenges}.


\subsection{Definition}
\label{sec:microservice-definition}

A microservice-based application is one in which the core functionality has been decomposed into many small units that can be independently developed and deployed \cite[p.~43]{khan2017key} \cite[p.~970]{vayghan2018deploying}. Each unit, a \textit{microservice}, is modeled around a single, clearly defined set of closely related functionalities that can be used independently over the network through a well-defined interface~\footnote{\citeauthor{cerny2018contextual} draw a comparison to three Unix ideas: a program should fulfill only one task well, be able to work with other programs and use a universal interface \cite[p.~31]{cerny2018contextual}.} \cite[p.~56]{taibi2018definition} \cite[p.~176]{vayghan2019microservice} \cite[p.~30]{cerny2018contextual}. This definition implies that microservices:

\begin{itemize}
  \item use independent codebases, thus can build on different technology stacks
  \item run in distinct processes, thus can fail and scale independently
  \item are decoupled but can be used as building blocks to form larger services
\end{itemize}


\subsection{Decomposition Techniques}
\label{sec:microservice-decomposition}

Spitting an application into services should happen along the lines of related processes that can be carried out in isolation. It is not about arbitrarily distributing features across services \cite[p.~61]{taibi2018definition}. As in traditional software engineering, the term cohesiveness is used to indicate that a service implements only functionalities strongly related to the concern that it is meant to model \cite[p.~2]{dragoni2017microservices}. Various techniques are cited to determine the breadth of concern:

\begin{description}
  \item[\acl{SRP}]
  \hfill \\
  Defines a responsibility of a class as a reason to change and states that a class should only have one such reason \cite[p.~36]{messina2016simplified} \cite[p.~116]{thones2015microservices}. Is analogously applied to microservices. Leads to a large amount of services.

  \item[Y-axis of \gls{scale cube}]
  \hfill \\
  Splits an application into distinct sets of related functions. Each set is implemented by a microservice. In a verb-based approach, sets consist of a single function that covers a specific use case, whereas the noun-based approach creates sets of functions responsible for all operations related to a particular entity \cite[p.~36]{messina2016simplified}. Leads to large and medium amount of services, respectively.

  \item[\acl{DDD}]
  \hfill \\
  Refers to the application's problem space, i.e. the business, as the domain. This domain consists of multiple subdomains (e.g. product catalog, order management). Each subdomain is represented by a microservice\todo{add citation} \cite[p.~3]{balalaie2016microservices}. Leads to a small amount of services.
\end{description}

Irrespective of the technique chosen, the overall goal should be to minimize later interface changes, i.e. to establish proper service contracts \cite[p.~26]{dmitry2014micro}.


\subsection{Service Registry Pattern}
\label{sec:service-registry-pattern}

The law of conservation of complexity states that the complexity of a large system does not vanish when the system is broken up into smaller pieces. Instead, the complexity is pushed to the interactions between these pieces \cite[p.~38]{messina2016simplified} \cite[p.~114]{thones2015microservices}. Applied to microservice-based applications, this means that developers need to deal with the challenges innate to distributed systems \cite[p.~24]{dmitry2014micro} \cite[p.~589]{villamizar2015evaluating}. One such challenge is the fact that services can no longer be invoked through language level method calls but rather only through the network. Moreover, given the need for scaling, clients are now required to make requests to a dynamically changing set of service instances. And since it is unfeasible to run these instances at fixed locations~\footnote{In the event of a network partition, a standard recovery process would attempt to restart a service in the healthy partition, thus leading to a new network location for this service.}, a pattern known as the service registry is commonly employed \cite[p.~37]{messina2016simplified}.

A service registry acts as a database of services, storing the various instances along their locations, i.e. the \acs{IP} address and port number. Instances are added on startup and removed on shutdown. Keeping this in mind, two types of service discovery mechanisms are distinguished \cite[p.~46]{khan2017key}:

\begin{description}
  \item[Client-side]
  \hfill \\
  To contact a service, clients obtain the locations of all service instances by querying the registry. The client then needs to perform a load balancing algorithm to decide which instance will be contacted.

  \item[Server-side]
  \hfill \\
  To contact a service, clients make a request to the service's load balancer which runs at a well known location. This load balancer queries the registry and forwards the request to an available instance.
\end{description}

In any case, the service registry is a critical component and thus, must be highly available.


\subsection{\acs{API} Gateway Pattern}
\label{sec:api-gateway-pattern}

Depending on the decomposition technique chosen (see~\autoref{sec:microservice-decomposition}), microservices might provide very fine-grained \acsp{API}. In turn, this means that clients may need to interact with multiple different services to carry out a high-level business process. To hide this complexity from clients and ensure consistent behavior, a pattern known as the \acs{API} gateway is employed.

An \acs{API} gateway represents the single entrypoint for all clients in which some requests are simply proxied while others fan out to and consume multiple services. In the latter case, gateways can be viewed as orchestrators and as such, typically do not have persistence layers \cite[p.~585]{villamizar2015evaluating}. They may, however, cache responses\todo{add citation}. Another important task in orchestrating multiple microservices is managing distributed transactions~\footnote{Distributed transactions are commonly implemented using the two-phase commit protocol. The no-\acs{ACID} transaction type has also been proposed for this context, which is known as a compensation transaction \cite[p.~32]{cerny2018contextual}.}, i.e. ensuring atomicity guarantees for a set of distributed resources \cite[p.~32]{cerny2018contextual}. Finally, gateways may also deal with generic features such as authentication and authorization or implement the circuit breaker pattern to prevent a service failure from cascading to other services \cite[p.~41]{kalske2017challenges} \cite[p.~37]{messina2016simplified}.

It shall be noted that gateways incur a performance penalty because they introduce an additional network hop \cite[p.~37]{messina2016simplified}. This is generally true for proxying gateways. Orchestrating gateways, on the other hand, have the potential to decrease latency since the various requests being collapsed now already originate from the target network\todo{add citation}. In both cases, the performance degradation will heavily depend on the system's interconnectedness \cite[p.~9]{dragoni2017microservices}.


\subsection{Database-per-Service Pattern}
\label{sec:database-per-service-pattern}

To keep microservices loosely coupled, a pattern known as database-per-service is employed. This pattern calls for each microservice to have its own database, compared to sharing one across multiple services. Sharing is achieved by making the data accessible via the service's \acs{API} \cite[p.~36]{messina2016simplified} \cite[p.~59]{taibi2018definition}. \citeauthor{messina2016simplified} discern between three levels of pattern conformity \cite[p.~37]{messina2016simplified}:

\begin{description}
  \item[Private tables]
  \hfill \\
  Each service has a set of tables private to that service.

  \item[Private schema]
  \hfill \\
  Each service has a database schema private to that service.

  \item[Private database]
  \hfill \\
  Each service has its own database.
\end{description}

While this pattern contributes to service intimacy, it comes at the cost of having to redefine data models and restate business rules across services~\footnote{In \acl{DDD}, the concept of a Bounded Context describes that services operate with business objects in a specific context and therefore, only need to model a subset of the global object's attributes \cite[p.~30]{cerny2018contextual}.} \cite[p.~30]{cerny2018contextual}.


\subsection{Delineation from \acl{SOA}}
\label{sec:soa-microservice-comparison}

Historically, the complexity of monolithic applications (see~\autoref{sec:monolith-problem}) has already been addressed using different \acf{SOA} approaches that also decompose a large system into many smaller services. Academia, however, is undecided whether microservices should be considered as a subset or superset of \acp{SOA} or whether it constitutes a new, distinct idea \cite[pp.~584--585]{villamizar2015evaluating} \cite[p.~30]{cerny2018contextual}. The systematic mapping study conducted by \citeauthor{cerny2018contextual} in \cite{cerny2018contextual} spends a great deal on contrasting the two architectures. A short summary is given in the following.

In both approaches, services cooperate to provide functionality for the overall system. However, the path to achieving this goal is different. This is most obvious when looking at the interaction patterns between the services involved. \acp{SOA} rely on orchestration, whereas microservice-based applications prefer choreography. The former expects a centralized business process to coordinate activities across services and combine the outcomes, whereas the latter expects individual services to collaborate based on their interface contracts. Orchestration differs from choreography with respect to where the logic that controls the interactions should reside. The two terms describe a centralized and decentralized approach hereof, respectively.

An important remark that \citeauthor{cerny2018contextual} make, is that orchestration through an integration layer, such as a messaging bus, oftentimes leads to a situation in which the system parties, i.e. the services, agree on a standardized representation of the business objects they exchange. The system ends up with one kind of business object each. This is known as a canonical data modal. The danger being that a change in one of the business objects necessitates changes in all of the services that deal with this object. As a result, deployments in a \ac{SOA} again happen in a monolithic fashion. In the microservice architecture, such a change can at most propagate to the \acs{API} gateways (comp.~\autoref{sec:api-gateway-pattern}), though this is less likely because gateways integrate services based on their interfaces, not on their models.

Lastly, albeit obvious, the biggest drawback of \acp{SOA} and their centralized orchestration model must be stated. Having a centralized business process integrate all services again entails having a single point of failure.


\section{OS-level Virtualization}
\label{sec:os-level-virtualization}

Virtualization describes the act of creating a virtual version of something \cite[p.~2]{celesti2016exploring}. In the context of computing, it specifically refers to hardware resources such as \acs{CPU}, memory, storage or network devices to create complete virtual instances of computer systems \cite[p.~21]{da2018containers}. This section sheds light on the motivation behind virtualization, gives a brief overview of the traditional approach hereto and focuses on a more recent and lightweight alternative in the remainder.


\subsection{The Need for Virtualization}
\label{sec:virtualization-motivation}

Server consolidation attempts to maximize resource utilization, while also reducing costs through energy savings~\footnote{About 10\% of the world's energy consumption stems from data center operations \cite[p.~1]{scheepers2014virtualization}. If resources can be used more efficiently, carbon emissions could be lowered.}\todo{update footnote figure and citation} \cite[p.~233]{xavier2013performance} \cite[p.~2]{eder2016hypervisor}. This is achieved by using fewer physical servers to host the same number of applications. But without an isolation layer there are no guarantees that an application from one user will not interfere with that of another \cite[p.~233]{xavier2013performance}. Isolation and multi-tenancy are exactly those traits promised by virtualization technologies \cite[p.~21]{da2018containers}. Coupled with a software layer to provision resources on demand, virtualization yields the elastic multi-tenant model embodied in \gls{cloud computing} \cite[p.~203]{kang2016container} \cite[p.~81]{bernstein2014containers} \cite[p.~24]{pahl2015containerization}.


\subsection{Comparison to Hardware Virtualization}
\label{sec:os-hardware-virtualization-comparison}

In traditional hardware virtualization, a so called hypervisor makes siloed slices of hardware available in the form of \acp{VM} by emulating the underlying physical resources. Each of the \acp{VM} (\textit{guests}) running on the physical hardware (\textit{host}) comes with its own full-fledged \acs{OS}. Two types of hypervisors can be discerned \cite[p.~2]{merkel2014docker} \cite[p.~1]{eder2016hypervisor} \cite[pp.~386--387]{morabito2015hypervisors}:

\begin{description}
  \item[Type 1 (bare-metal)]
  \hfill \\
  Operates directly on top of the host's hardware, not requiring a host \acs{OS}.

%  \textbf{Examples:} Xen, Hyper-V

  \item[Type 2 (hosted)]
  \hfill \\
  Operates as a software layer on top of the host's \acs{OS}.

%  \textbf{Examples:} KVM, VMware Workstation, VirtualBox
\end{description}

Although virtualization through emulation enjoys great popularity~\footnote{Hardware acceleration for hypervisor-based virtualization has even been incorporated into commodity processors \cite[p.~233]{xavier2013performance}.}, it comes at the cost of efficiency. On the other hand, the overhead introduced by OS-level virtualization can be considered almost negligible \cite[pp.~386,~392]{morabito2015hypervisors}.

Whereas hypervisor-based virtualization provides strong isolation guarantees between systems, \acs{OS}-level virtualization only strives to isolate processes. Such an isolated process is known as a container~\footnote{Containers are known as jails or zones in the FreeBSD and Solaris \acsp{OS}, respectively \cite[p.~2]{eder2016hypervisor}.}. Here, the isolation mechanisms are provided as kernel features that establish an abstract and protected view on the \acs{OS}, making two containers unaware of each other or any of the other processes running on the host \cite[p.~2]{merkel2014docker} \cite[pp.~1--2]{eder2016hypervisor}.

Even though both technologies enable a safe multi-tenant model of hardware by confining parts of the application infrastructure, \citeauthor{pahl2015containerization} ascribes them to different use cases, arguing that \acp{VM} are about hardware allocation and management, while containers are tools for delivering software. He further compares them to the concepts of \acs{IaaS} and \acs{PaaS}, respectively \cite[p.~24]{pahl2015containerization}. \citeauthor{eder2016hypervisor} and \citeauthor{merkel2014docker} see the potential for the two technologies to complement each other since the resource footprint of containers is minimal and its security profile is still slightly worse than that of \acsp{VM}~\footnote{Kernels cannot prevent interference in low-level resources such as the \acs{CPU}'s L3 cache or memory bandwidth \cite[p.~52]{burns2016borg}. A good example of combining both virtualization technologies is given by the Google Kubernetes Engine. Here, a cluster of hosts is created on the basis of \acsp{VM} which then exclusively runs software in the form of containers.}\todo{add citation}~\cite[p.~6]{eder2016hypervisor} \cite[p.~2]{merkel2014docker}.

Lastly, it shall be noted that the shared kernel approach of containers means that processes being isolated need to be compatible with the host's kernel and \acs{CPU} architecture \cite[p.~386]{morabito2015hypervisors} \cite[p.~2]{eder2016hypervisor}. In other words, it is not possible to, for example, run Windows containers on Linux hosts or deploy x86 containers on ARM because no emulation is taking place. On the flip side, the shared kernel approach allows kernel security patches to be applied without having to modify a container. This is not the case for a \acs{VM}, where the underlying machine image would have to be rebuild first \cite[p.~3]{eder2016hypervisor}. Moreover, sharing a kernel allows container-based solutions to achieve a higher density of virtualized instances when compared to hypervisor-based solutions \cite[p.~386]{morabito2015hypervisors} \cite[p.~204]{kang2016container}. Containers are also a magnitude faster to start and stop since they are essentially just processes that have to be spawned and terminated, whereas the \acs{OS} of a \acs{VM} needs to be fully booted and shut down \cite[p.~2]{merkel2014docker} \cite[p.~2]{eder2016hypervisor}. Similarly, being a process means that containers do not occupy any resources when they are not executing. \acsp{VM} will idle until they are shut down \cite[p.~2]{merkel2014docker}.


\subsection{Linux Kernel Containment Features}
\label{sec:linux-kernel-containment-features}

As previously hinted at, containers rely on the host's kernel to sandbox processes from each other. In Linux, the set of containment features include but are not limited to:

\begin{description}[format={\storedescriptionlabel}]
  \item[Chroots\label{itm:chroots}]
  \hfill \\
  \texttt{chroot()} (from \textit{ch}ange \textit{root}) changes the root directory of the calling process and all of its children~\footnote{Some people trace the inspiration for containers back to \texttt{chroot()}, which was originally introduced with Unix 7 in 1979 \cite[p.~82]{bernstein2014containers}.}. This is used to restrict a container's view on the filesystem. However, it cannot be considered a security feature because there are multiple intentional escape hatches\todo{update citation} \cite[p.~3]{eder2016hypervisor}.

  \item[Namespaces]
  \hfill \\
  Namespaces provide one or more processes with private and restricted views towards certain global system resources \cite[p.~387]{morabito2015hypervisors}. Changes to resources within a namespace are only visible to processes that are members of that namespace. The namespace type (e.g. \texttt{ipc}, \texttt{net}, \texttt{pid} or \texttt{user}) indicates which kinds of resources are being isolated\todo{update citation}. As an example, this feature allows processes within a container to have identifiers that are already in use on the host system or within other containers \cite[p.~3] {eder2016hypervisor}. The network namespace can provide a container with its own network device and virtual \acs{IP} address, whereas the user namespace would be used to ensure that a container's user database is separated from that of the host which means that the container's root user privileges cannot be applied on the host \cite[p.~1]{merkel2014docker}.

  \item[Control groups]
  \hfill \\
  Control groups, usually referred to as cgroups, provide resource accounting and limiting for a set of processes  \cite[p.~387]{morabito2015hypervisors} \cite[p.~1]{merkel2014docker}. Even though they are not mandatory for process isolation, cgroups can ensure that one container cannot starve another (e.g. in a \acs{DoS} attack), as well as to help realize cost-accounting multi-tenant environments \cite[p.~4]{eder2016hypervisor}.

  \item[\acl{MAC}]
  \hfill \\
  \ac{MAC} describes a concept in which access to or operations on a particular resource is granted based on different authorization rules (\textit{policies}). On Linux, the two prevalent implementations hereof are AppArmor and \acs{SELinux}~\footnote{\ac{SELinux} was originally developed by the \acs{NSA} to address threats of tampering and enable the confinement of damage caused by malicious applications.}\todo{add citation}. In the context of containers, \ac{MAC} is useful as that it can restrict the process to its minimal requirements, thus further reducing the attack surface against the host and other containers \cite[p.~4]{eder2016hypervisor}.
\end{description}


\subsection{Docker}
\label{sec:docker}

The concept of container-based virtualization has existed long before Docker came into view in 2013~\footnote{In 1998, FreeBSD came up with an extended version of \texttt{chroot()}, called jails. This capability further improved with the release of Solaris' zones in 2004 \cite[p.~82]{bernstein2014containers}. In Linux, kernel namespaces were discussed as early as 2006 \cite[p.~1]{celesti2016exploring}.}\todo{add citation}. Yet, it was Docker, Inc. who made containers popular by creating a toolkit that is greater than the sum of its parts \cite[p.~1]{merkel2014docker}. The following presents three areas of developer concern in which Docker shines and through which it has become a synonym for containers\todo{add citation}~\footnote{\citeauthor{boettiger2015introduction} makes a good case how Docker can also help to make research reproducible and more easily extendable \cite[p.~71]{boettiger2015introduction}.}:

\begin{description}
  \item[Code packaging]
  \hfill \\
  A study found that less than 50\% of software can be successfully built or installed.  This is due to issues such as the \enquote{dependency hell} problem (see~\autoref{sec:monolith-problem}), imprecise documentation or code rot\todo{add to glossary}. Executing code assumes the ability to create a compatible environment \cite[p.~72]{boettiger2015introduction}. Docker addresses this challenge with the concept of container images. Such an image bundles an application with all of its dependencies up to, but excluding, the kernel \cite[p.~1]{merkel2014docker}. It is essentially the filesystem bundle made available to a process that is then isolated as a container (comp.~\ref{itm:chroots} on \autopageref{itm:chroots}). To further simplify things, Docker allows users to imperatively describe how such an image shall be built. This plain text file of steps to be taken is known as a Dockerfile and is ideally suited for use with a \acs{VCS}, i.e. it can be checked in along a repository and evolve with the application \cite[p.~74]{boettiger2015introduction}. As a final innovation in this area, Docker calculates the filesystem differences between each of the steps in a Dockerfile and treats each as a distinct layer of the image. This enables developers to both version and extend images~\footnote{At runtime, all image layers will be merged into a single representation of the filesystem. This is known as a union mount \cite[p.~26]{pahl2015containerization}. } \cite[p.~1]{merkel2014docker}.

  \item[Code portability]
  \hfill \\
  By packaging an application along with its dependencies into a single image, Docker paves the way for image-based deployments that offer the freedom of \enquote{develop once, deploy everywhere}~\footnote{While image-based deployments can also be achieved with \acsp{VM}, they are not as portable nor lightweight because they contain the complete toolchain for running an \acs{OS}, including device drivers, the kernel and init system \cite[p.~203]{kang2016container} \cite[p.~2]{eder2016hypervisor}.} \cite[p.~203]{kang2016container}. In this context, a container can be regarded as a running instance of an image. However, it is unlikely that such a container by itself will run in the same way, if at all, across platforms due to differences in, for example, networking or storage \cite[pp.~74--75]{boettiger2015introduction}. This problem is known as runtime consistency \cite[p.~203]{kang2016container}. To enable consistent behavior, Docker ships with a container runtime that abstracts many of the platform peculiarities~\footnote{Of course, target platforms will still have to provide some sort of process isolation features (see~\autoref{sec:linux-kernel-containment-features}). Because this is not the case on macOS and Windows, Docker runs inside a Linux-based \acs{VM} on those platforms \cite[p.~5]{merkel2014docker}.} \cite[p.~75]{boettiger2015introduction}.

  \item[Code reuse]
  \hfill \\
  As previously stated, Docker allows one image to extend another. To make this process more straightforward, Docker offers a public registry, the Docker Hub, to and from which users can up- and download versioned, binary copies of images. \citeauthor{eder2016hypervisor} notes that having such a social aspect in the area of virtualization was unheard of before. At the same time, he warns that because images do not get updated automatically, there is a large amount of images containing security vulnerabilities. For instance, over 30\% of official images, i.e. those created by official project developers, contain high priority vulnerabilities \cite[pp.~6--7]{eder2016hypervisor}.
\end{description}

Driven by the risk of lack of interoperability, Docker and other leaders in the container industry established the \ac{OCI} in 2015 to standardize aforementioned container image formats and runtimes\todo{update citation} \cite[p.~23]{da2018containers}.


\section{Container Orchestration Platform}
\label{sec:container-orchestration-platform}

Given the operational benefits of microservice-based applications (see~\autoref{sec:microservice-architecture}), as well containers as a deployment target and medium (see~\autoref{sec:os-level-virtualization}), it is not surprising that the industry is focused on simplifying the management of potentially hundreds of instances of containerized services across a cluster of hosts (\textit{nodes}) \cite[p.~44]{khan2017key} \cite[p.~1]{pahl2017cloud}. This section presents the immediate benefits to dealing with container workloads, lists the requirements to any such container orchestration platform and tries to convey its way of working by laying out a reference architecture.


\subsection{The Shift to Container Workloads}
\label{sec:shift-to-container-workloads}

In an article about the lessons Google has learned from creating and operating three container-management systems over more than a decade~\footnote{Google has contributed much of the Linux kernel's process isolation code \cite[p.~50]{burns2016borg}.}, \citeauthor{burns2016borg} make a strong case how containerization transforms data centers from being machine oriented to being application oriented \cite[pp.~52--53]{burns2016borg}. Since a container is essentially just an isolated process, the identity of an instance being managed in a cluster now exactly lines up with the identity expected by developers, namely the main process (and its children) of their applications. This shift of primary key has ripple effects throughout the cluster's infrastructure. For instance, load balancers no longer balance traffic across machines but across instances of an application. Logs are automatically keyed by the application, whereas on machines logs are likely to be polluted by other applications or system operations. On the other hand, containers relieve developers and operations teams from worrying about machine-specific details. Together, this makes building, deploying, monitoring and debugging applications dramatically easier.


\subsection{Basic Capabilities}
\label{sec:container-orchestration-platform-capabilities}

A container orchestration platform can be broadly defined as a system that provides an enterprise-level framework for integrating and managing containers at scale \cite[p.~44]{khan2017key}. It is not only concerned with the runtime but also aids in the deploy and maintain phases of an application's lifecycle \cite[p.~225]{casalicchio2019container}. For the purposes of automation, all cluster operations should be exposed and accessible via an \acs{API} \cite[p.~224]{casalicchio2019container} \cite[p.~29]{pahl2015containerization}. Throughout literature, the following set of capabilities is generally expected from a container orchestration platform:

\begin{description}
  \item[Scheduling]
  \hfill \\
  Operators need to be able to deploy, i.e. schedule, containers onto hosts based on a variety of parameters. Typically, this encompasses a replication degree and placement constraints like \gls{node affinity} or resource requests \cite[p.~225]{casalicchio2019container}. Deployments should also be able to take advantage of local inter-process communication by allowing containers to be co-located on the same node. Once scheduled, the platform should perform a readiness check to decide whether the container is capable to answer requests. Similarly, periodic health checks should be used to determine whether a container needs to be restarted or rescheduled onto a different host \cite[p.~2]{al2019container}.

  \item[Scaling \& Availability]
  \hfill \\
  To cope with increases in traffic, horizontal scaling of applications is critical \cite[p.~1]{al2019container}. In this context, an application refers to one or multiple containers that are managed as a single entity \cite[p.~44]{khan2017key} \cite[p.~2]{pahl2017cloud}. Scaling may happen based on a static replication factor or through threshold-based autoscaling policies (e.g. \acs{CPU} or memory utilization). Platforms should also allow operators to define more sophisticated policies via external plug-ins. Naturally, the load then needs to be distributed amongst container instances by means of a load balancer that utilizes the health check mentioned above to decide whether to include a container instance in its target set or not \cite[pp.~225--226]{casalicchio2019container} \cite[p.~2]{al2019container}. However, scaling by itself does not guarantee high availability or fault tolerance. This is especially true when dealing with a single node cluster. Instead, operators should follow the three principles of reliability engineering in which failures need to be detected, single points of failure eliminated and reliable crossovers established \cite[p.~45]{khan2017key}. Of this, redundancy, i.e. replication across physically separated hosts, remains the most important feature, though self-healing by detecting failures is just as key. A container platform must support and implement both techniques \cite[p.~176]{vayghan2019microservice} \cite[p.~53]{burns2016borg}.

  \item[Monitoring]
  \hfill \\
  Platforms should facilitate monitoring across two contexts. First, the cluster infrastructure needs to be observed for resource utilization and draining. Secondly, container activity must be made visible by aggregating logs and performance metrics, as well as allowing \gls{white-box tracing} of requests \cite[p.~47]{khan2017key}.
\end{description}

Next to these basic capabilities, platforms may further implement a slew of features across domains such as security, networking or maintenance. As an example, platforms could provide role-based access control for containers, allow configuring \acl{MAC} (see~\autoref{sec:linux-kernel-containment-features}) on a per container basis, scan container images for vulnerabilities (see~\autoref{sec:docker}), add support for the service registry pattern (see~\autoref{sec:service-registry-pattern}) or enable cluster-wide backups.


\subsection{Reference Architecture}
\label{sec:orchestration-reference-architecture}

In a survey on the state of the art in container orchestration technologies, \citeauthor{casalicchio2019container} describes how a container orchestrator may be implemented as an autonomic computing system based on the \acs{MAPE-K} control loop \cite[pp.~226-228]{casalicchio2019container}. Such systems manage themselves by dynamically adapting to changes in accordance to business policies and objectives\todo{add citation}. For compute clusters, this typically involves goals such as maximizing resource usage, reducing energy consumption or satisfying \acsp{SLA}.

\begin{description}
  \item[Monitor]
  \hfill \\
  Collects details (e.g. node health or application load) from managed resources (e.g. \acsp{VM} or containers) and performs preliminary aggregation, correlation and filtering to determine whether a symptom needs to be further analyzed.

  \item[Analyzer]
  \hfill \\
  Performs more complex data analysis and reasoning on reported symptoms, this time considering the shared knowledge base on the system's topology, policies, historical logs and metrics.

  \item[Planner]
  \hfill \\
  Devises series of actions to be run against set of managed resources (e.g. spawn new container instances) in response to detected symptoms (e.g. high memory utilization).

  \item[Executor]
  \hfill \\
  Realizes adaptation plan using container and infrastructure management \acsp{API}.
\end{description}


\section{Agile Software Development}

\subsection{The Race to Market}
\subsection{\acl{CICD}}
\subsection{DevOps}
